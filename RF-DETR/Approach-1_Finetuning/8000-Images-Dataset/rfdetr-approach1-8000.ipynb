{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-28T08:58:33.710698Z",
     "iopub.status.busy": "2025-04-28T08:58:33.710000Z",
     "iopub.status.idle": "2025-04-28T08:58:48.974795Z",
     "shell.execute_reply": "2025-04-28T08:58:48.973734Z",
     "shell.execute_reply.started": "2025-04-28T08:58:33.710672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "working_dir = \"/kaggle/working\"\n",
    "\n",
    "for item in os.listdir(working_dir):\n",
    "    item_path = os.path.join(working_dir, item)\n",
    "\n",
    "    if os.path.isfile(item_path):\n",
    "        os.remove(item_path)\n",
    "    elif os.path.isdir(item_path):\n",
    "        shutil.rmtree(item_path)\n",
    "\n",
    "print(\"All contents within /kaggle/working have been deleted.\")\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:58:48.976297Z",
     "iopub.status.busy": "2025-04-28T08:58:48.976085Z",
     "iopub.status.idle": "2025-04-28T08:58:52.540436Z",
     "shell.execute_reply": "2025-04-28T08:58:52.539736Z",
     "shell.execute_reply.started": "2025-04-28T08:58:48.976280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pycocotools fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:58:52.541735Z",
     "iopub.status.busy": "2025-04-28T08:58:52.541416Z",
     "iopub.status.idle": "2025-04-28T08:58:52.548739Z",
     "shell.execute_reply": "2025-04-28T08:58:52.547851Z",
     "shell.execute_reply.started": "2025-04-28T08:58:52.541702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"/kaggle/input/8000-images-dataset/data.yaml\") as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "    class_names = data_config['names']  # Get real class names\n",
    "\n",
    "# Temporary workaround if you actually have 1 class\n",
    "if len(class_names) == 1:\n",
    "    class_names.append('background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:58:52.550939Z",
     "iopub.status.busy": "2025-04-28T08:58:52.550735Z",
     "iopub.status.idle": "2025-04-28T08:58:56.218749Z",
     "shell.execute_reply": "2025-04-28T08:58:56.217725Z",
     "shell.execute_reply.started": "2025-04-28T08:58:52.550924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rfdetr supervision albumentations opencv-python torch torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:58:56.220086Z",
     "iopub.status.busy": "2025-04-28T08:58:56.219848Z",
     "iopub.status.idle": "2025-04-28T08:59:13.633405Z",
     "shell.execute_reply": "2025-04-28T08:59:13.632509Z",
     "shell.execute_reply.started": "2025-04-28T08:58:56.220061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install -U albumentations\n",
    "!pip install rfdetr torchvision pytorch-lightning\n",
    "!pip install rfdetr[onnxexport]\n",
    "!pip install rfdetr supervision albumentations opencv-python torch torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T09:04:16.745855Z",
     "iopub.status.busy": "2025-04-28T09:04:16.745128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from rfdetr import RFDETRBase\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import albumentations as A\n",
    "\n",
    "# =====================\n",
    "# Configuration\n",
    "# =====================\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "yolo_dir = \"/kaggle/input/8000-images-dataset\"\n",
    "output_dir = \"/kaggle/working/coco_dataset\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =====================\n",
    "# Preprocessing Functions\n",
    "# =====================\n",
    "def adjust_brightness_contrast(image, alpha=1.2, beta=15, **kwargs):\n",
    "    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "def simple_white_balance(image, **kwargs):\n",
    "    result = image.astype(np.float32)\n",
    "    avg_b, avg_g, avg_r = np.mean(result, axis=(0,1))\n",
    "    avg_gray = (avg_b + avg_g + avg_r) / 3\n",
    "    result[..., 0] *= avg_gray / avg_b\n",
    "    result[..., 1] *= avg_gray / avg_g\n",
    "    result[..., 2] *= avg_gray / avg_r\n",
    "    return np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "def histogram_equalization(image, **kwargs):\n",
    "    img_yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "    img_yuv[..., 0] = cv2.equalizeHist(img_yuv[..., 0])\n",
    "    return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# =====================\n",
    "# Augmentation Pipeline\n",
    "# =====================\n",
    "preprocess = A.Compose([\n",
    "    A.Lambda(image=lambda image, **kw: adjust_brightness_contrast(image)),\n",
    "    A.Lambda(image=lambda image, **kw: simple_white_balance(image)),\n",
    "    A.Lambda(image=lambda image, **kw: histogram_equalization(image)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "])\n",
    "\n",
    "# =====================\n",
    "# Dataset Conversion\n",
    "# =====================\n",
    "def process_image(image_path):\n",
    "    \"\"\"Apply preprocessing pipeline\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    processed = preprocess(image=image)['image']\n",
    "    return processed\n",
    "\n",
    "def yolo_to_coco(yolo_root, output_root):\n",
    "    # Load class names\n",
    "    with open(os.path.join(yolo_root, \"data.yaml\")) as f:\n",
    "        class_names = yaml.safe_load(f)['names']\n",
    "    \n",
    "    if len(class_names) == 1:\n",
    "        class_names.append(\"background\")\n",
    "\n",
    "    # Create category mapping\n",
    "    categories = [{\"id\": i, \"name\": n, \"supercategory\": \"object\"} for i, n in enumerate(class_names)]\n",
    "\n",
    "    for split in ['train','valid','test']:\n",
    "        split_dir = os.path.join(output_root, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        coco_data = {\n",
    "            \"info\": {\"description\": \"Preprocessed Dataset\"},\n",
    "            \"licenses\": [{\"name\": \"MIT\"}],\n",
    "            \"categories\": categories,\n",
    "            \"images\": [],\n",
    "            \"annotations\": []\n",
    "        }\n",
    "\n",
    "        annotation_id = 1\n",
    "        image_id = 1\n",
    "\n",
    "        # Process images with preprocessing\n",
    "        img_dir = os.path.join(yolo_root, split, \"images\")\n",
    "        lbl_dir = os.path.join(yolo_root, split, \"labels\")\n",
    "\n",
    "        for img_file in tqdm(os.listdir(img_dir)):\n",
    "            if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                continue\n",
    "\n",
    "            # Process and save image\n",
    "            src_path = os.path.join(img_dir, img_file)\n",
    "            dst_path = os.path.join(split_dir, img_file)\n",
    "            \n",
    "            # Apply preprocessing chain\n",
    "            processed_img = process_image(src_path)\n",
    "            cv2.imwrite(dst_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            # Get dimensions\n",
    "            height, width = processed_img.shape[:2]\n",
    "\n",
    "            # Add image entry\n",
    "            coco_data[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": img_file,\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"license\": 1,\n",
    "                \"date_captured\": \"2024-01-01\"\n",
    "            })\n",
    "\n",
    "            # Process labels\n",
    "            label_path = os.path.join(lbl_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) != 5: continue\n",
    "\n",
    "                        cls_id, xc, yc, w, h = map(float, parts)\n",
    "                        x_min = (xc - w/2) * width\n",
    "                        y_min = (yc - h/2) * height\n",
    "                        box_w = w * width\n",
    "                        box_h = h * height\n",
    "\n",
    "                        coco_data[\"annotations\"].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": int(cls_id),\n",
    "                            \"bbox\": [x_min, y_min, box_w, box_h],\n",
    "                            \"area\": box_w * box_h,\n",
    "                            \"iscrowd\": 0\n",
    "                        })\n",
    "                        annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "        # Save annotations\n",
    "        with open(os.path.join(split_dir, \"_annotations.coco.json\"), 'w') as f:\n",
    "            json.dump(coco_data, f, indent=2)\n",
    "\n",
    "# =====================\n",
    "# Training Setup\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Convert dataset\n",
    "    import os\n",
    "    import shutil\n",
    "    \n",
    "    working_dir = \"/kaggle/working\"\n",
    "    \n",
    "    for item in os.listdir(working_dir):\n",
    "        item_path = os.path.join(working_dir, item)\n",
    "    \n",
    "        if os.path.isfile(item_path):\n",
    "            os.remove(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)\n",
    "    \n",
    "    print(\"All contents within /kaggle/working have been deleted.\")\n",
    "    yolo_to_coco(yolo_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T08:59:20.703241Z",
     "iopub.status.busy": "2025-04-28T08:59:20.703047Z",
     "iopub.status.idle": "2025-04-28T08:59:20.757147Z",
     "shell.execute_reply": "2025-04-28T08:59:20.756191Z",
     "shell.execute_reply.started": "2025-04-28T08:59:20.703226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from rfdetr import RFDETRBase  # adjust import as needed\n",
    "\n",
    "# Define output directory\n",
    "output_path = \"/kaggle/working/output\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Initialize model\n",
    "model = RFDETRBase(\n",
    "    resolution=336,\n",
    "    num_classes=len(class_names),\n",
    ")\n",
    "\n",
    "# Freeze the underlying PyTorch model inside RFDETRBase\n",
    "# Roboflow RFDETRBase wraps the real nn.Module in `model.network`\n",
    "module_to_freeze = model.network  # this is a torch.nn.Module\n",
    "\n",
    "# Freeze all parameters\n",
    "for name, param in module_to_freeze.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the classification and bbox heads\n",
    "for head_attr in ['class_embed', 'bbox_embed', 'head']:\n",
    "    head = getattr(module_to_freeze, head_attr, None)\n",
    "    if isinstance(head, torch.nn.Module):\n",
    "        for param in head.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# Optionally print trainable parameters for verification\n",
    "trainable = [name for name, param in module_to_freeze.named_parameters() if param.requires_grad]\n",
    "print(f\"Trainable parameters: {trainable}\")\n",
    "\n",
    "# Track best model\n",
    "best_val_acc = 0.0\n",
    "best_ckpt_path = os.path.join(output_path, \"best.pt\")\n",
    "\n",
    "# Metrics history\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "def on_epoch_end(metrics: dict):\n",
    "    global best_val_acc\n",
    "\n",
    "    epoch = len(history[\"epoch\"]) + 1\n",
    "    train_loss = metrics.get(\"train_loss\", 0.0)\n",
    "    val_loss = metrics.get(\"val_loss\", 0.0)\n",
    "    train_acc = metrics.get(\"train_map50_95\", 0.0)\n",
    "    val_acc = metrics.get(\"val_map50_95\", 0.0)\n",
    "\n",
    "    # Append to history\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        if os.path.exists(best_ckpt_path):\n",
    "            os.remove(best_ckpt_path)\n",
    "        torch.save(model.state_dict(), best_ckpt_path)\n",
    "        print(f\"[Epoch {epoch}] New best model saved to best.pt (val_acc={val_acc:.4f})\")\n",
    "\n",
    "# Register callback\n",
    "model.callbacks[\"on_fit_epoch_end\"].append(on_epoch_end)\n",
    "\n",
    "# Prepare trainer (unwrap DataParallel if used)\n",
    "trainer = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "\n",
    "# Train\n",
    "trainer.train(\n",
    "    dataset_dir=output_dir,\n",
    "    epochs=100,\n",
    "    batch_size=4,\n",
    "    grad_accum_steps=4,\n",
    "    lr=1e-4,\n",
    "    output_dir=output_path,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=5,  # updated from 15 to 5\n",
    "    tensorboard=True,\n",
    "    wandb=False,\n",
    "    optimizer_params={\"weight_decay\": 1e-4},\n",
    "    use_amp=True\n",
    ")\n",
    "\n",
    "print(\"Training completed successfully!\")\n",
    "\n",
    "# Save metrics to JSON and CSV\n",
    "with open(os.path.join(output_path, \"history.json\"), \"w\") as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "with open(os.path.join(output_path, \"history.csv\"), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"])\n",
    "    for i in range(len(history[\"epoch\"])):\n",
    "        writer.writerow([\n",
    "            history[\"epoch\"][i],\n",
    "            history[\"train_loss\"][i],\n",
    "            history[\"val_loss\"][i],\n",
    "            history[\"train_acc\"][i],\n",
    "            history[\"val_acc\"][i]\n",
    "        ])\n",
    "\n",
    "# Plotting\n",
    "epochs = history[\"epoch\"]\n",
    "\n",
    "# Loss plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs. Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_path, \"loss_vs_epoch.png\"))\n",
    "plt.show()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"Train Accuracy (mAP@.5:.95)\")\n",
    "plt.plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy (mAP@.5:.95)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_path, \"accuracy_vs_epoch.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-28T08:59:20.757814Z",
     "iopub.status.idle": "2025-04-28T08:59:20.758091Z",
     "shell.execute_reply": "2025-04-28T08:59:20.757988Z",
     "shell.execute_reply.started": "2025-04-28T08:59:20.757974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from rfdetr import RFDETRBase\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================\n",
    "# Configuration\n",
    "# =====================\n",
    "COCO_TEST_DIR = \"/kaggle/working/coco_dataset/test\"\n",
    "MODEL_PATH = \"/kaggle/input/ema-regular-model-8000-images/checkpoint_best_regular.pth\"\n",
    "IOU_THRESHOLD = 0.5\n",
    "OUTPUT_DIR = \"/kaggle/working/results\"\n",
    "ZIP_PATH = \"/kaggle/working/results.zip\"\n",
    "TILE_SIZE = (200, 200)\n",
    "\n",
    "print(\"Model exists:\", os.path.exists(MODEL_PATH))  # Should return True\n",
    "\n",
    "# =====================\n",
    "# Delete existing output\n",
    "# =====================\n",
    "for path in [OUTPUT_DIR, ZIP_PATH]:\n",
    "    if os.path.exists(path):\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "# =====================\n",
    "# COCO Annotation Loader\n",
    "# =====================\n",
    "def load_coco_annotations(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    id_to_image = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "    annotations = {}\n",
    "    \n",
    "    for ann in data[\"annotations\"]:\n",
    "        image_id = ann[\"image_id\"]\n",
    "        if image_id not in annotations:\n",
    "            annotations[image_id] = []\n",
    "        annotations[image_id].append(ann)\n",
    "    \n",
    "    return id_to_image, annotations\n",
    "\n",
    "# =====================\n",
    "# IoU Calculation\n",
    "# =====================\n",
    "def compute_iou(box1, box2):\n",
    "    xi1 = max(box1[0], box2[0])\n",
    "    yi1 = max(box1[1], box2[1])\n",
    "    xi2 = min(box1[2], box2[2])\n",
    "    yi2 = min(box1[3], box2[3])\n",
    "    \n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = (box1[2]-box1[0]) * (box1[3]-box1[1])\n",
    "    box2_area = (box2[2]-box2[0]) * (box2[3]-box2[1])\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "# =====================\n",
    "# Grid Creation Function\n",
    "# =====================\n",
    "def create_image_grid(image_paths, grid_path):\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, TILE_SIZE)\n",
    "            images.append(img)\n",
    "        else:\n",
    "            images.append(np.zeros((TILE_SIZE[1], TILE_SIZE[0], 3), dtype=np.uint8))\n",
    "    \n",
    "    while len(images) < 16:\n",
    "        images.append(np.zeros((TILE_SIZE[1], TILE_SIZE[0], 3), dtype=np.uint8))\n",
    "    \n",
    "    rows = []\n",
    "    for i in range(0, 16, 4):\n",
    "        row = np.hstack(images[i:i+4])\n",
    "        rows.append(row)\n",
    "    grid = np.vstack(rows)\n",
    "    cv2.imwrite(grid_path, grid)\n",
    "\n",
    "# =====================\n",
    "# Metrics Calculation\n",
    "# =====================\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    return precision, recall\n",
    "\n",
    "# =====================\n",
    "# Main Processing\n",
    "# =====================\n",
    "def analyze_predictions():\n",
    "    # Initialize model\n",
    "    model = RFDETRBase(checkpoint_path=MODEL_PATH)\n",
    "    \n",
    "    # Load COCO data\n",
    "    coco_json = os.path.join(COCO_TEST_DIR, \"_annotations.coco.json\")\n",
    "    id_to_image, coco_anns = load_coco_annotations(coco_json)\n",
    "    \n",
    "    # Create output directories\n",
    "    predicted_dir = os.path.join(OUTPUT_DIR, \"predicted\")\n",
    "    wrong_dir = os.path.join(OUTPUT_DIR, \"wrong\")\n",
    "    os.makedirs(predicted_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(wrong_dir, \"iou_threshold\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(wrong_dir, \"label_mismatch\"), exist_ok=True)\n",
    "    \n",
    "    # Metrics tracking\n",
    "    global_tp = 0\n",
    "    global_fp = 0\n",
    "    global_fn = 0\n",
    "    failure_data = []\n",
    "    \n",
    "    # Process each image\n",
    "    for image_id in tqdm(id_to_image, desc=\"Processing Images\"):\n",
    "        img_info = id_to_image[image_id]\n",
    "        img_path = os.path.join(COCO_TEST_DIR, img_info[\"file_name\"])\n",
    "        \n",
    "        # Load image and get predictions\n",
    "        image = Image.open(img_path)\n",
    "        detections = model.predict(image)\n",
    "        pred_boxes = np.array(detections.xyxy)\n",
    "        \n",
    "        # Get ground truth boxes\n",
    "        gt_boxes = []\n",
    "        if image_id in coco_anns:\n",
    "            for ann in coco_anns[image_id]:\n",
    "                x, y, w, h = ann[\"bbox\"]\n",
    "                gt_boxes.append([x, y, x+w, y+h])\n",
    "        gt_boxes = np.array(gt_boxes)\n",
    "        \n",
    "        # Create annotated image\n",
    "        processed_img = cv2.imread(img_path)\n",
    "        filename = os.path.basename(img_path)\n",
    "        \n",
    "        # Draw annotations\n",
    "        if gt_boxes.size > 0:\n",
    "            for gt in gt_boxes:\n",
    "                cv2.rectangle(processed_img, \n",
    "                            (int(gt[0]), int(gt[1])),\n",
    "                            (int(gt[2]), int(gt[3])),\n",
    "                            (0, 255, 0), 2)\n",
    "        \n",
    "        if pred_boxes.size > 0:\n",
    "            for box in pred_boxes:\n",
    "                cv2.rectangle(processed_img,\n",
    "                            (int(box[0]), int(box[1])),\n",
    "                            (int(box[2]), int(box[3])),\n",
    "                            (0, 0, 255), 2)\n",
    "        \n",
    "        # Save to predicted folder\n",
    "        cv2.imwrite(os.path.join(predicted_dir, filename), processed_img)\n",
    "        \n",
    "        # Initialize metrics for current image\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        \n",
    "        has_gt = gt_boxes.size > 0\n",
    "        has_pred = pred_boxes.size > 0\n",
    "        \n",
    "        if has_gt and has_pred:\n",
    "            cost_matrix = np.zeros((len(gt_boxes), len(pred_boxes)))\n",
    "            for i, gt in enumerate(gt_boxes):\n",
    "                for j, pred in enumerate(pred_boxes):\n",
    "                    cost_matrix[i, j] = -compute_iou(gt, pred)\n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            \n",
    "            matched_gt = set()\n",
    "            matched_pred = set()\n",
    "            \n",
    "            for i, j in zip(row_ind, col_ind):\n",
    "                iou_val = compute_iou(gt_boxes[i], pred_boxes[j])\n",
    "                if iou_val >= IOU_THRESHOLD:\n",
    "                    tp += 1\n",
    "                    matched_gt.add(i)\n",
    "                    matched_pred.add(j)\n",
    "            \n",
    "            fp += len(pred_boxes) - len(matched_pred)\n",
    "            fn += len(gt_boxes) - len(matched_gt)\n",
    "            \n",
    "        elif has_pred:\n",
    "            fp += len(pred_boxes)\n",
    "        elif has_gt:\n",
    "            fn += len(gt_boxes)\n",
    "        \n",
    "        # Update global metrics\n",
    "        global_tp += tp\n",
    "        global_fp += fp\n",
    "        global_fn += fn\n",
    "        \n",
    "        # Failure analysis\n",
    "        label_mismatch = (len(gt_boxes) != len(pred_boxes)) or (fp > 0) or (fn > 0)\n",
    "        low_iou = (tp > 0) and (fp + fn > 0)\n",
    "        \n",
    "        # Save failure cases\n",
    "        failure_reasons = []\n",
    "        if label_mismatch:\n",
    "            failure_reasons.append(\"label_mismatch\")\n",
    "            cv2.imwrite(os.path.join(wrong_dir, \"label_mismatch\", filename), processed_img)\n",
    "        if low_iou:\n",
    "            failure_reasons.append(\"iou_threshold\")\n",
    "            cv2.imwrite(os.path.join(wrong_dir, \"iou_threshold\", filename), processed_img)\n",
    "        \n",
    "        if failure_reasons:\n",
    "            failure_data.append({\n",
    "                \"image\": filename,\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn,\n",
    "                \"failure_reasons\": \", \".join(failure_reasons)\n",
    "            })\n",
    "\n",
    "    # Calculate final metrics\n",
    "    precision, recall = calculate_metrics(global_tp, global_fp, global_fn)\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics_content = f\"\"\"Evaluation Metrics:\n",
    "    - True Positives (TP): {global_tp}\n",
    "    - False Positives (FP): {global_fp}\n",
    "    - False Negatives (FN): {global_fn}\n",
    "    - Precision: {precision:.4f}\n",
    "    - Recall: {recall:.4f}\n",
    "    - F1 Score: {2*(precision*recall)/(precision+recall):.4f}\"\"\"\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, \"metrics.txt\"), \"w\") as f:\n",
    "        f.write(metrics_content)\n",
    "    \n",
    "    # Save CSV report\n",
    "    df = pd.DataFrame(failure_data)\n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"failure_report.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Create grids for wrong predictions\n",
    "    for subfolder in [\"iou_threshold\", \"label_mismatch\"]:\n",
    "        subfolder_path = os.path.join(wrong_dir, subfolder)\n",
    "        if os.path.exists(subfolder_path):\n",
    "            image_files = sorted([f for f in os.listdir(subfolder_path) \n",
    "                               if f.endswith('.jpg') and not f.startswith('grid_')])\n",
    "            \n",
    "            for i in range(0, len(image_files), 16):\n",
    "                chunk = image_files[i:i+16]\n",
    "                grid_path = os.path.join(subfolder_path, f\"grid_{i//16 + 1}.jpg\")\n",
    "                image_paths = [os.path.join(subfolder_path, f) for f in chunk]\n",
    "                create_image_grid(image_paths, grid_path)\n",
    "\n",
    "    # Create zip archive\n",
    "    shutil.make_archive(ZIP_PATH.rstrip('.zip'), 'zip', OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nâœ… Analysis complete!\")\n",
    "    print(f\"ðŸ“¦ Download results: {ZIP_PATH}\")\n",
    "    print(metrics_content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-28T08:59:20.759248Z",
     "iopub.status.idle": "2025-04-28T08:59:20.759489Z",
     "shell.execute_reply": "2025-04-28T08:59:20.759387Z",
     "shell.execute_reply.started": "2025-04-28T08:59:20.759373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n",
    "    os.chdir(directory)\n",
    "    zip_ref = zipfile.ZipFile(file_name, mode='w')\n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file_name in file:\n",
    "                pass\n",
    "            else:\n",
    "                zip_ref.write(os.path.join(folder, file))\n",
    "\n",
    "    return FileLink(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-28T08:59:20.760777Z",
     "iopub.status.idle": "2025-04-28T08:59:20.761011Z",
     "shell.execute_reply": "2025-04-28T08:59:20.760917Z",
     "shell.execute_reply.started": "2025-04-28T08:59:20.760906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "zip_dir(\"/kaggle/working/results\",'results-regular.zip')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6648494,
     "sourceId": 10724735,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7225120,
     "sourceId": 11520382,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
